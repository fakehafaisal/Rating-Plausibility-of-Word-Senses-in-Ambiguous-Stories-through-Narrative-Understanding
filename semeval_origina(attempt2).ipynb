{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "H35UcNwz1d-P"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -q sentence-transformers scipy"
      ],
      "metadata": {
        "id": "_ONWKieK5cZ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create Codabench-style folders\n",
        "!mkdir -p input/ref input/res output\n",
        "# move the reference (ground truth) file\n",
        "!mkdir -p input/ref input/res output\n",
        "!mv solution.jsonl input/ref/solution.jsonl\n",
        "!mv predictions.jsonl input/res/predictions.jsonl\n",
        "!echo \"âœ… Folder structure:\"\n",
        "!tree -L 2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozg5mEV-7fBn",
        "outputId": "26f8d257-d8a8-47e4-a68e-b8db0c9c20eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mv: cannot stat 'solution.jsonl': No such file or directory\n",
            "mv: cannot stat 'predictions.jsonl': No such file or directory\n",
            "âœ… Folder structure:\n",
            "/bin/bash: line 1: tree: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()   # click \"Choose files\" and select the files (you can multi-select)\n",
        "print(\"Uploaded:\", list(uploaded.keys()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "_k68hJnX6Tk1",
        "outputId": "c8e262d5-dd83-4ad6-bf71-82490b4ffa43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d8d02fac-fb31-48c3-a9fc-171a34d42ec0\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d8d02fac-fb31-48c3-a9fc-171a34d42ec0\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving dev.json to dev.json\n",
            "Saving evaluate.py to evaluate.py\n",
            "Saving format_check.py to format_check.py\n",
            "Saving scoring.py to scoring.py\n",
            "Saving solution.jsonl to solution.jsonl\n",
            "Uploaded: ['dev.json', 'evaluate.py', 'format_check.py', 'scoring.py', 'solution.jsonl']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create folders\n",
        "import os\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "os.makedirs(\"input/ref\", exist_ok=True)\n",
        "os.makedirs(\"input/res\", exist_ok=True)\n",
        "os.makedirs(\"output\", exist_ok=True)\n",
        "\n",
        "# If you don't have data files in the workspace, upload them now:\n",
        "print(\"If you already uploaded data/dev.json and data/train.json, ignore the upload prompt.\")\n",
        "from google.colab import files\n",
        "uploaded = files.upload()  # use the file chooser to upload train.json and dev.json if needed\n",
        "print(\"Uploaded:\", list(uploaded.keys()))\n",
        "\n",
        "# If you uploaded a solution.jsonl file here and want it in input/ref:\n",
        "if \"solution.jsonl\" in uploaded:\n",
        "    os.replace(\"solution.jsonl\", \"input/ref/solution.jsonl\")\n",
        "    print(\"Moved solution.jsonl -> input/ref/solution.jsonl\")\n",
        "\n",
        "print(\"\\nCurrent data folder contents:\")\n",
        "!ls -la data || true\n",
        "print(\"\\ninput/ref contents:\")\n",
        "!ls -la input/ref || true\n",
        "print(\"\\ninput/res contents:\")\n",
        "!ls -la input/res || true"
      ],
      "metadata": {
        "id": "tnnqNQwb_VJ2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "909adfc3-2981-4c65-9fc6-04e7588d58dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "If you already uploaded data/dev.json and data/train.json, ignore the upload prompt.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b21af916-c76c-4ce9-abe2-e90fb5d2359c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b21af916-c76c-4ce9-abe2-e90fb5d2359c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving train_aug.json to train_aug.json\n",
            "Uploaded: ['train_aug.json']\n",
            "\n",
            "Current data folder contents:\n",
            "total 8\n",
            "drwxr-xr-x 2 root root 4096 Nov 22 08:27 .\n",
            "drwxr-xr-x 1 root root 4096 Nov 22 08:28 ..\n",
            "\n",
            "input/ref contents:\n",
            "total 8\n",
            "drwxr-xr-x 2 root root 4096 Nov 22 08:26 .\n",
            "drwxr-xr-x 4 root root 4096 Nov 22 08:26 ..\n",
            "\n",
            "input/res contents:\n",
            "total 8\n",
            "drwxr-xr-x 2 root root 4096 Nov 22 08:26 .\n",
            "drwxr-xr-x 4 root root 4096 Nov 22 08:26 ..\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json, pandas as pd, numpy as np, torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.stats import spearmanr\n",
        "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
        "import torch.nn.functional as F\n",
        "\n",
        "MODEL_DIR = \"./sbert-ambi\"\n",
        "TRAIN_FILE = \"data/train_aug.json\"\n",
        "DEV_FILE = \"data/dev.json\"\n",
        "\n",
        "# -----------------------------\n",
        "# Build story helper\n",
        "# -----------------------------\n",
        "def build_story(row):\n",
        "    sentence = str(row[\"sentence\"]).replace(row[\"homonym\"], f\"[TGT] {row['homonym']} [TGT]\", 1)\n",
        "    parts = [str(row.get(\"precontext\", \"\")), sentence]\n",
        "    if row.get(\"ending\", \"\") not in [None, \"\"]:\n",
        "        parts.append(str(row.get(\"ending\", \"\")))\n",
        "    return \" \".join([p for p in parts if p])\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Load pretrained model or train\n",
        "# -----------------------------\n",
        "if os.path.exists(MODEL_DIR) and os.listdir(MODEL_DIR):\n",
        "    print(f\"Found saved model at {MODEL_DIR}, loading it.\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\n",
        "    base_model = AutoModel.from_pretrained(MODEL_DIR)\n",
        "\n",
        "else:\n",
        "    # -----------------------------\n",
        "    # Load training data\n",
        "    # -----------------------------\n",
        "    with open(TRAIN_FILE, \"r\") as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    records = list(data.values()) if isinstance(data, dict) else data\n",
        "    df = pd.DataFrame(records)\n",
        "\n",
        "    # Split\n",
        "    train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n",
        "    train_df = train_df.reset_index(drop=True)\n",
        "    val_df = val_df.reset_index(drop=True)\n",
        "\n",
        "    # Build story + normalize labels\n",
        "    train_df[\"story\"] = train_df.apply(build_story, axis=1)\n",
        "    val_df[\"story\"] = val_df.apply(build_story, axis=1)\n",
        "    train_df[\"average_norm\"] = (train_df[\"average\"] - 1.0) / 4.0\n",
        "    val_df[\"average_norm\"] = (val_df[\"average\"] - 1.0) / 4.0\n",
        "\n",
        "    # -----------------------------\n",
        "    # Use MPNet model\n",
        "    # -----------------------------\n",
        "    model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    base_model = AutoModel.from_pretrained(model_name, output_hidden_states=True)\n",
        "\n",
        "    # -----------------------------\n",
        "    # Dataset class\n",
        "    # -----------------------------\n",
        "    class WordPairDataset(Dataset):\n",
        "        def __init__(self, df):\n",
        "            self.data = df\n",
        "        def __len__(self): return len(self.data)\n",
        "        def __getitem__(self, idx):\n",
        "            row = self.data.iloc[idx]\n",
        "            return {\n",
        "                \"story\": row[\"story\"],\n",
        "                \"example_sentence\": row[\"example_sentence\"],\n",
        "                \"homonym\": row[\"homonym\"],\n",
        "                \"label\": torch.tensor(float(row[\"average_norm\"]), dtype=torch.float)\n",
        "            }\n",
        "\n",
        "    train_dataset = WordPairDataset(train_df)\n",
        "    val_dataset = WordPairDataset(val_df)\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=8)\n",
        "    val_dataloader = DataLoader(val_dataset, shuffle=False, batch_size=8)\n",
        "\n",
        "    # -----------------------------\n",
        "    # Training setup\n",
        "    # -----------------------------\n",
        "    optimizer = torch.optim.AdamW(base_model.parameters(), lr=2e-5)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    base_model.to(device)\n",
        "\n",
        "    EPOCHS = 10\n",
        "    num_training_steps = len(train_dataloader) * EPOCHS\n",
        "\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=int(0.1 * num_training_steps),\n",
        "        num_training_steps=num_training_steps\n",
        "    )\n",
        "\n",
        "    # -----------------------------\n",
        "    # Proper homonym embedding extractor\n",
        "    # -----------------------------\n",
        "    def get_target_embedding(text, homonym):\n",
        "      inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
        "\n",
        "      with torch.set_grad_enabled(base_model.training):\n",
        "          outputs = base_model(**inputs)\n",
        "\n",
        "      # --- Get all hidden states (list of layer outputs) ---\n",
        "      # outputs.hidden_states: list of 25 layers for MPNet\n",
        "      hidden_states = outputs.hidden_states\n",
        "\n",
        "      # Mean of last 4 layers (much more stable for semantic tasks)\n",
        "      stacked = torch.stack(hidden_states[-8:])      # shape: [4, batch, seq, 768]\n",
        "      hidden = torch.mean(stacked, dim=0)[0]         # shape: [seq, 768]\n",
        "\n",
        "      tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "      indices = [i for i, t in enumerate(tokens) if homonym.lower() in t.lower()]\n",
        "\n",
        "      if len(indices) == 0:\n",
        "          # fallback to whole-sentence mean\n",
        "          return hidden.mean(dim=0)\n",
        "\n",
        "      # mean pooling over homonym subwords\n",
        "      emb = hidden[indices].mean(dim=0)\n",
        "      return emb\n",
        "\n",
        "\n",
        "    # -----------------------------\n",
        "    # TRAINING LOOP (fixed + optimized)\n",
        "    # -----------------------------\n",
        "    print(\"ðŸš€ Starting training...\")\n",
        "\n",
        "    best_spearman = -1\n",
        "    patience_counter = 0\n",
        "    PATIENCE = 2\n",
        "\n",
        "    for epoch in range(1, EPOCHS + 1):\n",
        "        base_model.train()\n",
        "        epoch_loss = 0\n",
        "\n",
        "        # --- Training ---\n",
        "        for batch in train_dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            losses = []\n",
        "\n",
        "            for i in range(len(batch[\"story\"])):\n",
        "                emb_story = get_target_embedding(batch[\"story\"][i], batch[\"homonym\"][i])\n",
        "                emb_examp = get_target_embedding(batch[\"example_sentence\"][i], batch[\"homonym\"][i])\n",
        "\n",
        "                emb_story = F.normalize(emb_story, p=2, dim=0)\n",
        "                emb_examp = F.normalize(emb_examp, p=2, dim=0)\n",
        "\n",
        "                cos_sim = F.cosine_similarity(emb_story, emb_examp, dim=0)\n",
        "                loss = (cos_sim - batch[\"label\"][i].to(device)) ** 2\n",
        "                losses.append(loss)\n",
        "\n",
        "            loss_batch = torch.stack(losses).mean()\n",
        "            loss_batch.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            epoch_loss += loss_batch.item()\n",
        "\n",
        "        print(f\"Epoch {epoch} | Train Loss: {epoch_loss / len(train_dataloader):.4f}\")\n",
        "\n",
        "        # --- Validation ---\n",
        "        base_model.eval()\n",
        "        all_sims, all_labels = [], []\n",
        "\n",
        "        with torch.no_grad():  # no gradient computation for validation\n",
        "            for batch in val_dataloader:\n",
        "                batch_labels = batch[\"label\"].numpy()\n",
        "                all_labels.extend(batch_labels)\n",
        "                sims = []\n",
        "\n",
        "                for i in range(len(batch[\"story\"])):\n",
        "                    emb_story = get_target_embedding(batch[\"story\"][i], batch[\"homonym\"][i])\n",
        "                    emb_examp = get_target_embedding(batch[\"example_sentence\"][i], batch[\"homonym\"][i])\n",
        "\n",
        "                    emb_story = F.normalize(emb_story, p=2, dim=0)\n",
        "                    emb_examp = F.normalize(emb_examp, p=2, dim=0)\n",
        "\n",
        "                    sim = F.cosine_similarity(emb_story, emb_examp, dim=0).item()\n",
        "                    sims.append(sim)\n",
        "\n",
        "                all_sims.extend(sims)\n",
        "\n",
        "        # --- Metrics ---\n",
        "        val_spearman, _ = spearmanr(all_sims, all_labels)\n",
        "        labels_scaled = (np.array(all_labels) * 4) + 1\n",
        "        sims_scaled = ((np.array(all_sims) + 1) / 2) * 4 + 1\n",
        "        val_acc = np.mean(np.abs(sims_scaled - labels_scaled) <= 1.0)\n",
        "\n",
        "        print(f\"Epoch {epoch} | Spearman: {val_spearman:.4f}, Acc_within_1.0: {val_acc:.4f}\")\n",
        "\n",
        "        # --- Early Stopping ---\n",
        "        if val_spearman > best_spearman:\n",
        "            best_spearman = val_spearman\n",
        "            patience_counter = 0\n",
        "            print(f\"ðŸ’¾ New best model! Saving checkpoint.\")\n",
        "            torch.save(base_model.state_dict(), \"best_model.pt\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            print(f\"âš  No improvement. Patience: {patience_counter}/{PATIENCE}\")\n",
        "            if patience_counter >= PATIENCE:\n",
        "                print(\"â›” Early stopping triggered. Training halted.\")\n",
        "                break\n",
        "\n",
        "\n",
        "\n",
        "        # Spearman correlation\n",
        "        spearman_corr = spearmanr(np.array(all_labels), np.array(all_sims)).correlation\n",
        "\n",
        "        # Accuracy within 1.0 (converted to 1â€“5 scale)\n",
        "        labels_scaled = (np.array(all_labels) * 4) + 1\n",
        "        sims_scaled = ((np.array(all_sims) + 1) / 2) * 4 + 1\n",
        "        acc_std = np.mean(np.abs(sims_scaled - labels_scaled) <= 1.0)\n",
        "\n",
        "        print(f\"Epoch {epoch} | Spearman: {spearman_corr:.4f}, Acc_within_1.0: {acc_std:.4f}\")\n",
        "\n",
        "    # -----------------------------\n",
        "    # SAVE MODEL\n",
        "    # -----------------------------\n",
        "    os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "    base_model.save_pretrained(MODEL_DIR)\n",
        "    tokenizer.save_pretrained(MODEL_DIR)\n",
        "\n",
        "    print(\"âœ… Training complete. Model saved to:\", MODEL_DIR)\n",
        "\n",
        "print(\"Model ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30MacIBnASDy",
        "outputId": "52250cba-6c66-456c-b642-ef5e5a037bed",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ Starting training...\n",
            "Epoch 1 | Train Loss: 0.0770\n",
            "Epoch 1 | Spearman: 0.6512, Acc_within_1.0: 0.5123\n",
            "ðŸ’¾ New best model! Saving checkpoint.\n",
            "Epoch 1 | Spearman: 0.6512, Acc_within_1.0: 0.5123\n",
            "Epoch 2 | Train Loss: 0.0385\n",
            "Epoch 2 | Spearman: 0.7638, Acc_within_1.0: 0.5123\n",
            "ðŸ’¾ New best model! Saving checkpoint.\n",
            "Epoch 2 | Spearman: 0.7638, Acc_within_1.0: 0.5123\n",
            "Epoch 3 | Train Loss: 0.0222\n",
            "Epoch 3 | Spearman: 0.8288, Acc_within_1.0: 0.5263\n",
            "ðŸ’¾ New best model! Saving checkpoint.\n",
            "Epoch 3 | Spearman: 0.8288, Acc_within_1.0: 0.5263\n",
            "Epoch 4 | Train Loss: 0.0163\n",
            "Epoch 4 | Spearman: 0.8284, Acc_within_1.0: 0.5088\n",
            "âš  No improvement. Patience: 1/2\n",
            "Epoch 4 | Spearman: 0.8284, Acc_within_1.0: 0.5088\n",
            "Epoch 5 | Train Loss: 0.0121\n",
            "Epoch 5 | Spearman: 0.8405, Acc_within_1.0: 0.5158\n",
            "ðŸ’¾ New best model! Saving checkpoint.\n",
            "Epoch 5 | Spearman: 0.8405, Acc_within_1.0: 0.5158\n",
            "Epoch 6 | Train Loss: 0.0100\n",
            "Epoch 6 | Spearman: 0.8353, Acc_within_1.0: 0.5228\n",
            "âš  No improvement. Patience: 1/2\n",
            "Epoch 6 | Spearman: 0.8353, Acc_within_1.0: 0.5228\n",
            "Epoch 7 | Train Loss: 0.0086\n",
            "Epoch 7 | Spearman: 0.8323, Acc_within_1.0: 0.5053\n",
            "âš  No improvement. Patience: 2/2\n",
            "â›” Early stopping triggered. Training halted.\n",
            "âœ… Training complete. Model saved to: ./sbert-ambi\n",
            "Model ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 4 â€” Generate predictions.jsonl using your trained word-level model\n",
        "import os, json, torch, numpy as np\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch.nn.functional as F # Import F for torch.nn.functional alias\n",
        "\n",
        "MODEL_PATH = \"./sbert-ambi\"\n",
        "DATA_PATH = \"data/dev.json\"          # switch to data/test.json when submitting\n",
        "OUT_PATH  = \"input/res/predictions.jsonl\"\n",
        "\n",
        "# Load fine-tuned model and tokenizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
        "model = AutoModel.from_pretrained(MODEL_PATH, output_hidden_states=True).to(device)\n",
        "model.eval()\n",
        "\n",
        "# Helper: build story text (same as before)\n",
        "def build_story(row):\n",
        "    # This must match the structure used in training!\n",
        "    sentence = str(row[\"sentence\"]).replace(row[\"homonym\"], f\"[TGT] {row['homonym']} [TGT]\", 1)\n",
        "    parts = [str(row.get(\"precontext\", \"\")), str(row.get(\"sentence\", \"\"))]\n",
        "    if row.get(\"ending\", \"\") not in [None, \"\"]:\n",
        "        parts.append(str(row.get(\"ending\", \"\")))\n",
        "    return \" \".join([p for p in parts if p])\n",
        "\n",
        "# Function to extract contextual embedding for the target word (MUST match Step 3 logic!)\n",
        "def get_target_embedding(text, homonym):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Get all hidden states (list of layers)\n",
        "    hidden_states = outputs.hidden_states\n",
        "\n",
        "    # Stack last 8 layers\n",
        "    stacked = torch.stack(hidden_states[-8:])  # shape: [8, batch, seq, hidden]\n",
        "    hidden = torch.mean(stacked, dim=0)[0]     # shape: [seq, hidden]\n",
        "\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
        "    indices = [i for i, t in enumerate(tokens) if homonym.lower() in t.lower()]\n",
        "\n",
        "    if len(indices) == 0:\n",
        "        return hidden.mean(dim=0)\n",
        "\n",
        "    emb = hidden[indices].mean(dim=0)\n",
        "    return emb\n",
        "\n",
        "\n",
        "# Load dataset\n",
        "with open(DATA_PATH, \"r\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "records = list(data.values()) if isinstance(data, dict) else data\n",
        "\n",
        "predictions = []\n",
        "# Ensure unique IDs are maintained as per the input data (which uses string keys)\n",
        "for key, sample in data.items():\n",
        "    story = build_story(sample)\n",
        "    example_sentence = sample[\"example_sentence\"]\n",
        "    homonym = sample[\"homonym\"]\n",
        "\n",
        "    # Get target word embeddings\n",
        "    emb_story = get_target_embedding(story, homonym)\n",
        "    emb_example = get_target_embedding(example_sentence, homonym)\n",
        "\n",
        "    # Compute cosine similarity between the two embeddings\n",
        "    # Using F.cosine_similarity from the imported alias\n",
        "    sim = F.cosine_similarity(emb_story, emb_example, dim=0).item()\n",
        "\n",
        "    # Map similarity (-1..1) â†’ 1..5\n",
        "    score = ((sim + 1) / 2) * 4 + 1\n",
        "    score_int = int(round(score))\n",
        "    score_int = min(5, max(1, score_int))  # clamp to [1,5]\n",
        "\n",
        "    predictions.append({\"id\": key, \"prediction\": score_int}) # Use original key as ID\n",
        "\n",
        "# Save predictions.jsonl\n",
        "os.makedirs(os.path.dirname(OUT_PATH), exist_ok=True)\n",
        "with open(OUT_PATH, \"w\") as f:\n",
        "    for p in predictions:\n",
        "        f.write(json.dumps(p) + \"\\n\")\n",
        "\n",
        "print(f\"âœ… Saved {len(predictions)} predictions to {OUT_PATH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NB8le78XBUe6",
        "outputId": "3fae8798-b5e0-46b1-c799-b58bc0d1344a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Saved 588 predictions to input/res/predictions.jsonl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 scoring.py input/ref/solution.jsonl input/res/predictions.jsonl output/scores.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8KF7V4YBhkL",
        "outputId": "2074736a-30b2-4975-8db1-c36bbbcd1294"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Importing...\n",
            "Starting Scoring script...\n",
            "Everything looks OK. Evaluating file input/res/predictions.jsonl on input/ref/solution.jsonl\n",
            "----------\n",
            "Spearman Correlation: 0.4009626490796731\n",
            "Spearman p-Value: 4.054177562458891e-24\n",
            "----------\n",
            "Accuracy: 0.5629251700680272 (331/588)\n",
            "Results dumped into scores.json successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -j my_submission.zip input/res/predictions.jsonl\n",
        "from google.colab import files\n",
        "files.download(\"my_submission.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "lCtcypq5BtKv",
        "outputId": "88b8c6dd-a755-404c-8ff7-b9dc0b89db46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: predictions.jsonl (deflated 91%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_1cf40a3e-f3a4-4c1b-b295-64028b82ed8f\", \"my_submission.zip\", 1903)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------------------------------------------------------"
      ],
      "metadata": {
        "id": "_PRrSG0cB_oq"
      }
    }
  ]
}